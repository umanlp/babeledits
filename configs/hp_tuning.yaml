defaults:
  - model: llama-3-1
  - method: ft
  - decoding_strategy: greedy
  - _self_

metrics: 
  - token_em_lm_eval
  - rewrite_score
locality_metrics:
  - nkl
dataset: "datasets/babeledits_test.json"
edit_lang: "en"
tgt_langs: ['ar', 'de', 'en', 'fr', 'hr', 'it', 'ja', 'ka', 'my', 'qu', 'zh']
multi_answer_eval: true
max_edits: null
device: 0
log_subdir: "new_debug"
prompt_type: "prompts_mt_marked"
subject_type: "subjects_mt_marked"
target_type: "targets_mt_marked"
eval_prompt_type: ["prompts_mt_marked"]
subject_in_prompt: null
generality: false
locality: false
portability: false
eval_lm: false
data_lm : "data_ppl/ME-PPL_50_translated.json"
batch_size_lm: 32
lm_metric: "ppl"
pre_file: null
pre_edit: null
pre_eval_only: false
wandb:
  project_name: "babeledits-hp_tuning"
sequential: false
log_on_wandb: true